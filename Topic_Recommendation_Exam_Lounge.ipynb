{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Recommendation_Exam Lounge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6M/90QHFRIwCK7gCiJt1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushkal1234/Topic-Recommendation-System_E-learning-Platform/blob/main/Topic_Recommendation_Exam_Lounge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5akF-jS2iIC"
      },
      "source": [
        "\n",
        "A Collaborative Filtering Approach Towards Implementation of Topic Recommendation System for an E-Learning Platform ExamLounge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUjiFx5vo3cB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86993c4-ba18-476e-9fb6-c62b958b4dc8"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"../../\")\n",
        "import os\n",
        "!{sys.executable} -m pip install surprise\n",
        "!{sys.executable} -m pip install papermill \n",
        "import papermill as pm\n",
        "!{sys.executable} -m pip install scrapbook\n",
        "import scrapbook as sb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "import re\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "from collections import defaultdict"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting surprise\n",
            "  Downloading https://files.pythonhosted.org/packages/61/de/e5cba8682201fcf9c3719a6fdda95693468ed061945493dea2dd37c5618b/surprise-0.1-py2.py3-none-any.whl\n",
            "Collecting scikit-surprise\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/37/5d334adaf5ddd65da99fc65f6507e0e4599d092ba048f4302fe8775619e8/scikit-surprise-1.1.1.tar.gz (11.8MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8MB 227kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1617631 sha256=1b00946c9fbc2504883680254c719f82d3b8655a0f145c67a9f644d4ea560a47\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/9c/3d/41b419c9d2aff5b6e2b4c0fc8d25c538202834058f9ed110d0\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n",
            "Collecting papermill\n",
            "  Downloading https://files.pythonhosted.org/packages/ec/3b/55dbb2017142340a57937f1fad78c7d9373552540b42740e37fd6c40d761/papermill-2.3.3-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.7/dist-packages (from papermill) (4.41.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from papermill) (3.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from papermill) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from papermill) (2.23.0)\n",
            "Collecting tenacity\n",
            "  Downloading https://files.pythonhosted.org/packages/41/ee/d6eddff86161c6a3a1753af4a66b06cbc508d3b77ca4698cd0374cd66531/tenacity-7.0.0-py2.py3-none-any.whl\n",
            "Collecting ansiwrap\n",
            "  Downloading https://files.pythonhosted.org/packages/03/50/43e775a63e0d632d9be3b3fa1c9b2cbaf3b7870d203655710a3426f47c26/ansiwrap-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from papermill) (5.1.3)\n",
            "Collecting black\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d0/154973fbb48aeda17cd117507872079de82408bb16f6f2ead3d05be68bd6/black-21.6b0-py3-none-any.whl (140kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 33.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from papermill) (0.5.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from papermill) (0.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->papermill) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->papermill) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->papermill) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->papermill) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from tenacity->papermill) (1.15.0)\n",
            "Collecting textwrap3>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/77/9c/a53e561d496ee5866bbeea4d3a850b3b545ed854f8a21007c1e0d872e94d/textwrap3-0.9.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill) (5.0.5)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill) (4.7.1)\n",
            "Collecting typed-ast>=1.4.2; python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 33.1MB/s \n",
            "\u001b[?25hCollecting pathspec<1,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/29/29/a465741a3d97ea3c17d21eaad4c64205428bde56742360876c4391f930d4/pathspec-0.8.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black->papermill) (1.4.4)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Collecting regex>=2020.1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/28/5f08d8841013ccf72cd95dfff2500fe7fb39467af12c5e7b802d8381d811/regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720kB)\n",
            "\u001b[K     |████████████████████████████████| 727kB 27.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from black->papermill) (3.7.4.3)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black->papermill) (0.10.2)\n",
            "Requirement already satisfied: async-generator in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill) (1.10)\n",
            "Collecting jupyter-client>=6.1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/e8/c3cf72a32a697256608d5fa96360c431adec6e1c6709ba7f13f99ff5ee04/jupyter_client-6.1.12-py3-none-any.whl (112kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill) (1.5.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill) (22.1.0)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill) (5.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill) (2.8.1)\n",
            "Installing collected packages: tenacity, textwrap3, ansiwrap, typed-ast, pathspec, mypy-extensions, regex, black, papermill, jupyter-client\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "Successfully installed ansiwrap-0.8.4 black-21.6b0 jupyter-client-6.1.12 mypy-extensions-0.4.3 papermill-2.3.3 pathspec-0.8.1 regex-2021.4.4 tenacity-7.0.0 textwrap3-0.9.2 typed-ast-1.4.3\n",
            "Collecting scrapbook\n",
            "  Downloading https://files.pythonhosted.org/packages/27/dc/68f9c96997dffbf3632bebe0d88077a519aa2a74585834e84d6690243825/scrapbook-0.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scrapbook) (1.1.5)\n",
            "Requirement already satisfied: papermill in /usr/local/lib/python3.7/dist-packages (from scrapbook) (2.3.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from scrapbook) (3.0.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from scrapbook) (5.5.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from scrapbook) (2.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (2.8.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.3)\n",
            "Requirement already satisfied: ansiwrap in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.8.4)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (7.0.0)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (21.6b0)\n",
            "Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (7.1.2)\n",
            "Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.5.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (3.13)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (5.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (2.23.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (5.0.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (57.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->scrapbook) (1.15.0)\n",
            "Requirement already satisfied: textwrap3>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from ansiwrap->papermill->scrapbook) (0.9.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (0.4.3)\n",
            "Requirement already satisfied: pathspec<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (0.8.1)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (3.7.4.3)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (2021.4.4)\n",
            "Requirement already satisfied: typed-ast>=1.4.2; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (1.4.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (1.5.1)\n",
            "Requirement already satisfied: jupyter-client>=6.1.5 in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (6.1.12)\n",
            "Requirement already satisfied: async-generator in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (1.10)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (4.7.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (2021.5.30)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->scrapbook) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->scrapbook) (0.7.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (22.1.0)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (5.1.1)\n",
            "Installing collected packages: scrapbook\n",
            "Successfully installed scrapbook-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xauSj0pNYfWp"
      },
      "source": [
        "We will use the ExamLounge dataset, which is composed of integer user level ratings from 1 to 5.\n",
        "\n",
        "**Dataset:** We have 2 Dataset csv files named as topics.csv and level_ratings.csv\n",
        "\n",
        "**\"ExamLounge_Topics.csv\":** It contains 3 Columns named as topicId, topic_title and exam &\n",
        "45,756 Rows. \n",
        "\n",
        "**\"LevelRatings.csv\":** It contains 4 Columns named as userId,  topicId, rating, timestamp & 15,323 Rows. \n",
        "\n",
        "PS: The Data is taken from the Exam Lounge: Ed.Tech Startup in this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvZBex-OYNgG"
      },
      "source": [
        "class ExamLounge:\n",
        "    topicId_to_name = {}\n",
        "    name_to_topicId = {}\n",
        "    ratingsPath = '/content/LevelRatings.csv'\n",
        "    topicsPath = '/content/ExamLounge_Topics.csv'\n",
        "    \n",
        "    def loadExamLoungeLatestSmall(self):\n",
        "\n",
        "        # Look for files relative to the directory we are running from\n",
        "        os.chdir(os.path.dirname(sys.argv[0]))\n",
        "\n",
        "        ratingsDataset = 0\n",
        "        self.topicId_to_name = {}\n",
        "        self.name_to_topicId = {}\n",
        "\n",
        "        reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
        "\n",
        "        ratingsDataset = Dataset.load_from_file(self.ratingsPath, reader=reader)\n",
        "\n",
        "        with open(self.topicsPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "                topicReader = csv.reader(csvfile)\n",
        "                next(topicReader)  #Skip header line\n",
        "                for row in topicReader:\n",
        "                    topicId = int(row[0])\n",
        "                    topicName = row[1]\n",
        "                    self.topicId_to_name[topicId] = topicName\n",
        "                    self.name_to_topicId[topicName] = topicId\n",
        "\n",
        "        return ratingsDataset\n",
        "    def getUserRatings(self, user):\n",
        "        userRatings = []\n",
        "        hitUser = False\n",
        "        with open(self.ratingsPath, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile)\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                userID = int(row[0])\n",
        "                if (user == userID):\n",
        "                    topicId = int(row[1])\n",
        "                    rating = float(row[2])\n",
        "                    userRatings.append((topicId, rating))\n",
        "                    hitUser = True\n",
        "                if (hitUser and (user != userID)):\n",
        "                    break\n",
        "\n",
        "        return userRatings\n",
        "\n",
        "    def getPopularityRanks(self):\n",
        "        ratings = defaultdict(int)\n",
        "        rankings = defaultdict(int)\n",
        "        with open(self.ratingsPath, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile)\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                topicId = int(row[1])\n",
        "                ratings[topicId] += 1\n",
        "        rank = 1\n",
        "        for topicId, ratingCount in sorted(ratings.items(), key=lambda x: x[1], reverse=True):\n",
        "            rankings[topicId] = rank\n",
        "            rank += 1\n",
        "        return rankings\n",
        "    \n",
        "    def getexam(self):\n",
        "        exam = defaultdict(list)\n",
        "        genreIDs = {}\n",
        "        maxGenreID = 0\n",
        "        with open(self.topicsPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            topicReader = csv.reader(csvfile)\n",
        "            next(topicReader)  #Skip header line\n",
        "            for row in topicReader:\n",
        "                topicId = int(row[0])\n",
        "                genreList = row[2].split('|')\n",
        "                genreIDList = []\n",
        "                for genre in genreList:\n",
        "                    if genre in genreIDs:\n",
        "                        genreID = genreIDs[genre]\n",
        "                    else:\n",
        "                        genreID = maxGenreID\n",
        "                        genreIDs[genre] = genreID\n",
        "                        maxGenreID += 1\n",
        "                    genreIDList.append(genreID)\n",
        "                exam[topicId] = genreIDList\n",
        "        # Convert integer-encoded genre lists to bitfields that we can treat as vectors\n",
        "        for (topicId, genreIDList) in exam.items():\n",
        "            bitfield = [0] * maxGenreID\n",
        "            for genreID in genreIDList:\n",
        "                bitfield[genreID] = 1\n",
        "            exam[topicId] = bitfield            \n",
        "        \n",
        "        return exam\n",
        "    \n",
        "    def getYears(self):\n",
        "        p = re.compile(r\"(?:\\((\\d{4})\\))?\\s*$\")\n",
        "        years = defaultdict(int)\n",
        "        with open(self.topicsPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            topicReader = csv.reader(csvfile)\n",
        "            next(topicReader)\n",
        "            for row in topicReader:\n",
        "                topicId = int(row[0])\n",
        "                topic_title = row[1]\n",
        "                m = p.search(topic_title)\n",
        "                year = m.group(1)\n",
        "                if year:\n",
        "                    years[topicId] = int(year)\n",
        "        return years\n",
        "    \n",
        "    def getMiseEnScene(self):\n",
        "        mes = defaultdict(list)\n",
        "        with open(\"LLVisualFeatures13K_Log.csv\", newline='') as csvfile:\n",
        "            mesReader = csv.reader(csvfile)\n",
        "            next(mesReader)\n",
        "            for row in mesReader:\n",
        "                topicId = int(row[0])\n",
        "                avgShotLength = float(row[1])\n",
        "                meanColorVariance = float(row[2])\n",
        "                stddevColorVariance = float(row[3])\n",
        "                meanMotion = float(row[4])\n",
        "                stddevMotion = float(row[5])\n",
        "                meanLightingKey = float(row[6])\n",
        "                numShots = float(row[7])\n",
        "                mes[topicId] = [avgShotLength, meanColorVariance, stddevColorVariance,\n",
        "                   meanMotion, stddevMotion, meanLightingKey, numShots]\n",
        "        return mes\n",
        "    \n",
        "    def gettopicName(self, topicId):\n",
        "        if topicId in self.topicId_to_name:\n",
        "            return self.topicId_to_name[topicId]\n",
        "        else:\n",
        "            return \"\"\n",
        "        \n",
        "    def gettopicId(self, topicName):\n",
        "        if topicName in self.name_to_topicId:\n",
        "            return self.name_to_topicId[topicName]\n",
        "        else:\n",
        "            return 0    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYAO2jwebAeI"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "\n",
        "class RecommenderMetrics:\n",
        "\n",
        "    def MAE(predictions):\n",
        "        return accuracy.mae(predictions, verbose=False)\n",
        "\n",
        "    def RMSE(predictions):\n",
        "        return accuracy.rmse(predictions, verbose=False)\n",
        "\n",
        "    def GetTopN(predictions, n=10, minimumRating=4.0):\n",
        "        topN = defaultdict(list)\n",
        "\n",
        "\n",
        "        for userID, topicId, actualRating, estimatedRating, _ in predictions:\n",
        "            if (estimatedRating >= minimumRating):\n",
        "                topN[int(userID)].append((int(topicId), estimatedRating))\n",
        "\n",
        "        for userID, ratings in topN.items():\n",
        "            ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "            topN[int(userID)] = ratings[:n]\n",
        "\n",
        "        return topN\n",
        "\n",
        "    def HitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for leftOut in leftOutPredictions:\n",
        "            userID = leftOut[0]\n",
        "            leftOuttopicId = leftOut[1]\n",
        "            # Is it in the predicted top 10 for this user?\n",
        "            hit = False\n",
        "            for topicId, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOuttopicId) == int(topicId)):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits += 1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOuttopicId, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Only look at ability to recommend things the users actually liked...\n",
        "            if (actualRating >= ratingCutoff):\n",
        "                # Is it in the predicted top 10 for this user?\n",
        "                hit = False\n",
        "                for topicId, predictedRating in topNPredicted[int(userID)]:\n",
        "                    if (int(leftOuttopicId) == topicId):\n",
        "                        hit = True\n",
        "                        break\n",
        "                if (hit) :\n",
        "                    hits += 1\n",
        "\n",
        "                total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def RatingHitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = defaultdict(float)\n",
        "        total = defaultdict(float)\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOuttopicId, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hit = False\n",
        "            for topicId, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOuttopicId) == topicId):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits[actualRating] += 1\n",
        "\n",
        "            total[actualRating] += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        for rating in sorted(hits.keys()):\n",
        "            print (rating, hits[rating] / total[rating])\n",
        "\n",
        "    def AverageReciprocalHitRank(topNPredicted, leftOutPredictions):\n",
        "        summation = 0\n",
        "        total = 0\n",
        "        # For each left-out rating\n",
        "        for userID, leftOuttopicId, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hitRank = 0\n",
        "            rank = 0\n",
        "            for topicId, predictedRating in topNPredicted[int(userID)]:\n",
        "                rank = rank + 1\n",
        "                if (int(leftOuttopicId) == topicId):\n",
        "                    hitRank = rank\n",
        "                    break\n",
        "            if (hitRank > 0) :\n",
        "                summation += 1.0 / hitRank\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        return summation / total\n",
        "\n",
        "    # What percentage of users have at least one \"good\" recommendation\n",
        "    def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):\n",
        "        hits = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            hit = False\n",
        "            for topicId, predictedRating in topNPredicted[userID]:\n",
        "                if (predictedRating >= ratingThreshold):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit):\n",
        "                hits += 1\n",
        "\n",
        "        return hits / numUsers\n",
        "\n",
        "    def Diversity(topNPredicted, simsAlgo):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        simsMatrix = simsAlgo.compute_similarities()\n",
        "        for userID in topNPredicted.keys():\n",
        "            pairs = itertools.combinations(topNPredicted[userID], 2)\n",
        "            for pair in pairs:\n",
        "                topic1 = pair[0][0]\n",
        "                topic2 = pair[1][0]\n",
        "                innerID1 = simsAlgo.trainset.to_inner_iid(str(topic1))\n",
        "                innerID2 = simsAlgo.trainset.to_inner_iid(str(topic2))\n",
        "                similarity = simsMatrix[innerID1][innerID2]\n",
        "                total += similarity\n",
        "                n += 1\n",
        "\n",
        "        S = total / n\n",
        "        return (1-S)\n",
        "\n",
        "    def Novelty(topNPredicted, rankings):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            for rating in topNPredicted[userID]:\n",
        "                topicId = rating[0]\n",
        "                rank = rankings[topicId]\n",
        "                total += rank\n",
        "                n += 1\n",
        "        return total / n\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8UU1BYqbzbt"
      },
      "source": [
        "from surprise.model_selection import train_test_split\n",
        "from surprise.model_selection import LeaveOneOut\n",
        "from surprise import KNNBaseline\n",
        "\n",
        "\n",
        "class EvaluationData:\n",
        "    \n",
        "    def __init__(self, data, popularityRankings):\n",
        "        \n",
        "        self.rankings = popularityRankings\n",
        "        #Build a full training set for evaluating overall properties\n",
        "        self.fullTrainSet = data.build_full_trainset()\n",
        "        self.fullAntiTestSet = self.fullTrainSet.build_anti_testset()\n",
        "        \n",
        "        #Build a 75/25 train/test split for measuring accuracy\n",
        "        self.trainSet, self.testSet = train_test_split(data, test_size=.25, random_state=1)\n",
        "        \n",
        "        #Build a \"leave one out\" train/test split for evaluating top-N recommenders\n",
        "        #And build an anti-test-set for building predictions\n",
        "        LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n",
        "        for train, test in LOOCV.split(data):\n",
        "            self.LOOCVTrain = train\n",
        "            self.LOOCVTest = test\n",
        "            \n",
        "        self.LOOCVAntiTestSet = self.LOOCVTrain.build_anti_testset()\n",
        "        \n",
        "        #Compute similarty matrix between items so we can measure diversity\n",
        "        sim_options = {'name': 'cosine', 'user_based': False}\n",
        "        self.simsAlgo = KNNBaseline(sim_options=sim_options)\n",
        "        self.simsAlgo.fit(self.fullTrainSet)\n",
        "            \n",
        "    def GetFullTrainSet(self):\n",
        "        return self.fullTrainSet\n",
        "    \n",
        "    def GetFullAntiTestSet(self):\n",
        "        return self.fullAntiTestSet\n",
        "    \n",
        "    def GetAntiTestSetForUser(self, testSubject):\n",
        "        trainset = self.fullTrainSet\n",
        "        fill = trainset.global_mean\n",
        "        anti_testset = []\n",
        "        u = trainset.to_inner_uid(str(testSubject))\n",
        "        user_items = set([j for (j, _) in trainset.ur[u]])\n",
        "        anti_testset += [(trainset.to_raw_uid(u), trainset.to_raw_iid(i), fill) for\n",
        "                                 i in trainset.all_items() if\n",
        "                                 i not in user_items]\n",
        "        return anti_testset\n",
        "\n",
        "    def GetTrainSet(self):\n",
        "        return self.trainSet\n",
        "    \n",
        "    def GetTestSet(self):\n",
        "        return self.testSet\n",
        "    \n",
        "    def GetLOOCVTrainSet(self):\n",
        "        return self.LOOCVTrain\n",
        "    \n",
        "    def GetLOOCVTestSet(self):\n",
        "        return self.LOOCVTest\n",
        "    \n",
        "    def GetLOOCVAntiTestSet(self):\n",
        "        return self.LOOCVAntiTestSet\n",
        "    \n",
        "    def GetSimilarities(self):\n",
        "        return self.simsAlgo\n",
        "    \n",
        "    def GetPopularityRankings(self):\n",
        "        return self.rankings"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T48DolOy1e73"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgguhS1G7Oxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79b59c5-2e46-4307-ce46-20965aded832"
      },
      "source": [
        "# %run ./ExamLounge.ipynb\n",
        "from surprise import KNNBasic\n",
        "import heapq\n",
        "from collections import defaultdict\n",
        "from operator import itemgetter\n",
        "from surprise.model_selection import LeaveOneOut\n",
        "# %run ./RecommenderMetrics.ipynb\n",
        "# %run ./EvaluationData.ipynb\n",
        "\n",
        "def LoadExamLoungeData():\n",
        "    ml = ExamLounge()\n",
        "    print(\"Loading topic ratings...\")\n",
        "    data = ml.loadExamLoungeLatestSmall()\n",
        "    print(\"\\nComputing topic popularity ranks so we can measure novelty later...\")\n",
        "    rankings = ml.getPopularityRanks()\n",
        "    return (ml, data, rankings)\n",
        "\n",
        "ml, data, rankings = LoadExamLoungeData()\n",
        "\n",
        "evalData = EvaluationData(data, rankings)\n",
        "\n",
        "# Train on leave-One-Out train set\n",
        "trainSet = evalData.GetLOOCVTrainSet()\n",
        "sim_options = {'name': 'cosine',\n",
        "               'user_based': True\n",
        "               }\n",
        "\n",
        "model = KNNBasic(sim_options=sim_options)\n",
        "model.fit(trainSet)\n",
        "simsMatrix = model.compute_similarities()\n",
        "\n",
        "leftOutTestSet = evalData.GetLOOCVTestSet()\n",
        "\n",
        "# Build up dict to lists of (int(topicId), predictedrating) pairs\n",
        "topN = defaultdict(list)\n",
        "k = 10\n",
        "for uiid in range(trainSet.n_users):\n",
        "    # Get top N similar users to this one\n",
        "    similarityRow = simsMatrix[uiid]\n",
        "    \n",
        "    similarUsers = []\n",
        "    for innerID, score in enumerate(similarityRow):\n",
        "        if (innerID != uiid):\n",
        "            similarUsers.append( (innerID, score) )\n",
        "    \n",
        "    kNeighbors = heapq.nlargest(k, similarUsers, key=lambda t: t[1])\n",
        "    \n",
        "    # Get the stuff they rated, and add up ratings for each item, weighted by user similarity\n",
        "    candidates = defaultdict(float)\n",
        "    for similarUser in kNeighbors:\n",
        "        innerID = similarUser[0]\n",
        "        userSimilarityScore = similarUser[1]\n",
        "        theirRatings = trainSet.ur[innerID]\n",
        "        for rating in theirRatings:\n",
        "            candidates[rating[0]] += (rating[1] / 5.0) * userSimilarityScore\n",
        "        \n",
        "    # Build a dictionary of stuff the user has already seen\n",
        "    watched = {}\n",
        "    for itemID, rating in trainSet.ur[uiid]:\n",
        "        watched[itemID] = 1\n",
        "        \n",
        "    # Get top-rated items from similar users:\n",
        "    pos = 0\n",
        "    for itemID, ratingSum in sorted(candidates.items(), key=itemgetter(1), reverse=True):\n",
        "        if not itemID in watched:\n",
        "            topicId = trainSet.to_raw_iid(itemID)\n",
        "            topN[int(trainSet.to_raw_uid(uiid))].append( (int(topicId), 0.0) )\n",
        "            pos += 1\n",
        "            if (pos > 40):\n",
        "                break\n",
        "    \n",
        "# Measure\n",
        "print(\"HR\", RecommenderMetrics.HitRate(topN, leftOutTestSet))   \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading topic ratings...\n",
            "\n",
            "Computing topic popularity ranks so we can measure novelty later...\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "HR 0.01443298969072165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7PL5XTg7O95"
      },
      "source": [
        "#!pip install RecommenderMetrics\n",
        "#!pip install EvaluationData\n",
        "\n",
        "class EvaluatedAlgorithm:\n",
        "    \n",
        "    def __init__(self, algorithm, name):\n",
        "        self.algorithm = algorithm\n",
        "        self.name = name\n",
        "        \n",
        "    def Evaluate(self, evaluationData, doTopN, n=10, verbose=True):\n",
        "        metrics = {}\n",
        "        # Compute accuracy\n",
        "        if (verbose):\n",
        "            print(\"Evaluating accuracy...\")\n",
        "        self.algorithm.fit(evaluationData.GetTrainSet())\n",
        "        predictions = self.algorithm.test(evaluationData.GetTestSet())\n",
        "        metrics[\"RMSE\"] = RecommenderMetrics.RMSE(predictions)\n",
        "        metrics[\"MAE\"] = RecommenderMetrics.MAE(predictions)\n",
        "        \n",
        "        if (doTopN):\n",
        "            # Evaluate top-10 with Leave One Out testing\n",
        "            if (verbose):\n",
        "                print(\"Evaluating top-N with leave-one-out...\")\n",
        "            self.algorithm.fit(evaluationData.GetLOOCVTrainSet())\n",
        "            leftOutPredictions = self.algorithm.test(evaluationData.GetLOOCVTestSet())        \n",
        "            # Build predictions for all ratings not in the training set\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetLOOCVAntiTestSet())\n",
        "            # Compute top 10 recs for each user\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Computing hit-rate and rank metrics...\")\n",
        "            # See how often we recommended a topic the user actually rated\n",
        "            metrics[\"HR\"] = RecommenderMetrics.HitRate(topNPredicted, leftOutPredictions)   \n",
        "            # See how often we recommended a topic the user actually liked\n",
        "            metrics[\"cHR\"] = RecommenderMetrics.CumulativeHitRate(topNPredicted, leftOutPredictions)\n",
        "            # Compute ARHR\n",
        "            metrics[\"ARHR\"] = RecommenderMetrics.AverageReciprocalHitRank(topNPredicted, leftOutPredictions)\n",
        "        \n",
        "            #Evaluate properties of recommendations on full training set\n",
        "            if (verbose):\n",
        "                print(\"Computing recommendations with full data set...\")\n",
        "            self.algorithm.fit(evaluationData.GetFullTrainSet())\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetFullAntiTestSet())\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Analyzing coverage, diversity, and novelty...\")\n",
        "            # Print user coverage with a minimum predicted rating of 4.0:\n",
        "            metrics[\"Coverage\"] = RecommenderMetrics.UserCoverage(  topNPredicted, \n",
        "                                                                   evaluationData.GetFullTrainSet().n_users, \n",
        "                                                                   ratingThreshold=4.0)\n",
        "            # Measure diversity of recommendations:\n",
        "            metrics[\"Diversity\"] = RecommenderMetrics.Diversity(topNPredicted, evaluationData.GetSimilarities())\n",
        "            \n",
        "            # Measure novelty (average popularity rank of recommendations):\n",
        "            metrics[\"Novelty\"] = RecommenderMetrics.Novelty(topNPredicted, \n",
        "                                                            evaluationData.GetPopularityRankings())\n",
        "        \n",
        "        if (verbose):\n",
        "            print(\"Analysis complete.\")\n",
        "    \n",
        "        return metrics\n",
        "    \n",
        "    def GetName(self):\n",
        "        return self.name\n",
        "    \n",
        "    def GetAlgorithm(self):\n",
        "        return self.algorithm\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L53g39HD7PDc"
      },
      "source": [
        "#!pip install EvaluationData\n",
        "#!pip install EvaluatedAlgorithm\n",
        "\n",
        "class Evaluator:\n",
        "    \n",
        "    algorithms = []\n",
        "    \n",
        "    def __init__(self, dataset, rankings):\n",
        "        ed = EvaluationData(dataset, rankings)\n",
        "        self.dataset = ed\n",
        "        \n",
        "    def AddAlgorithm(self, algorithm, name):\n",
        "        alg = EvaluatedAlgorithm(algorithm, name)\n",
        "        self.algorithms.append(alg)\n",
        "        \n",
        "    def Evaluate(self, doTopN):\n",
        "        results = {}\n",
        "        for algorithm in self.algorithms:\n",
        "            print(\"Evaluating \", algorithm.GetName(), \"...\")\n",
        "            results[algorithm.GetName()] = algorithm.Evaluate(self.dataset, doTopN)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        if (doTopN):\n",
        "            print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "                    \"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"cHR\", \"ARHR\", \"Coverage\", \"Diversity\", \"Novelty\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
        "                        name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"cHR\"], metrics[\"ARHR\"],\n",
        "                                      metrics[\"Coverage\"], metrics[\"Diversity\"], metrics[\"Novelty\"]))\n",
        "        else:\n",
        "            print(\"{:<10} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f}\".format(name, metrics[\"RMSE\"], metrics[\"MAE\"]))\n",
        "                \n",
        "        print(\"\\nLegend:\\n\")\n",
        "        print(\"RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\")\n",
        "        print(\"MAE:       Mean Absolute Error. Lower values mean better accuracy.\")\n",
        "        if (doTopN):\n",
        "            print(\"HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\")\n",
        "            print(\"cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\")\n",
        "            print(\"ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\" )\n",
        "            print(\"Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\")\n",
        "            print(\"Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\")\n",
        "            print(\"           for a given user. Higher means more diverse.\")\n",
        "            print(\"Novelty:   Average popularity rank of recommended items. Higher means more novel.\")\n",
        "        \n",
        "    def SampleTopNRecs(self, ml, testSubject=89, k=10):\n",
        "        \n",
        "        for algo in self.algorithms:\n",
        "            print(\"\\nUsing recommender \", algo.GetName())\n",
        "            \n",
        "            print(\"\\nBuilding recommendation model...\")\n",
        "            trainSet = self.dataset.GetFullTrainSet()\n",
        "            algo.GetAlgorithm().fit(trainSet)\n",
        "            \n",
        "            print(\"Computing recommendations...\")\n",
        "            testSet = self.dataset.GetAntiTestSetForUser(testSubject)\n",
        "        \n",
        "            predictions = algo.GetAlgorithm().test(testSet)\n",
        "            \n",
        "            recommendations = []\n",
        "            \n",
        "            print (\"\\nWe recommend:\")\n",
        "            for userID, topicId, actualRating, estimatedRating, _ in predictions:\n",
        "                inttopicId = int(topicId)\n",
        "                recommendations.append((inttopicId, estimatedRating))\n",
        "            \n",
        "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "            \n",
        "            for ratings in recommendations[:10]:\n",
        "                print(ml.gettopicName(ratings[0]), ratings[1])\n",
        "                \n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNjiQjqw7PGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "537219cb-587d-40b7-a5fe-10d639d59181"
      },
      "source": [
        "#!pip install ExamLounge\n",
        "from surprise import KNNBasic\n",
        "from surprise import NormalPredictor\n",
        "#!pip install Evaluator\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def LoadExamLoungeData():\n",
        "    ml = ExamLounge()\n",
        "    print(\"Loading topic ratings...\")\n",
        "    data = ml.loadExamLoungeLatestSmall()\n",
        "    print(\"\\nComputing topic popularity ranks so we can measure novelty later...\")\n",
        "    rankings = ml.getPopularityRanks()\n",
        "    return (ml, data, rankings)\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Load up common data set for the recommender algorithms\n",
        "(ml, evaluationData, rankings) = LoadExamLoungeData()\n",
        "\n",
        "# Construct an Evaluator to, you know, evaluate them\n",
        "evaluator = Evaluator(evaluationData, rankings)\n",
        "\n",
        "# User-based KNN\n",
        "UserKNN = KNNBasic(sim_options = {'name': 'cosine', 'user_based': True})\n",
        "evaluator.AddAlgorithm(UserKNN, \"User KNN\")\n",
        "\n",
        "# Item-based KNN\n",
        "ItemKNN = KNNBasic(sim_options = {'name': 'cosine', 'user_based': False})\n",
        "evaluator.AddAlgorithm(ItemKNN, \"Item KNN\")\n",
        "\n",
        "# Just make random recommendations\n",
        "Random = NormalPredictor()\n",
        "evaluator.AddAlgorithm(Random, \"Random\")\n",
        "\n",
        "# Fight!\n",
        "evaluator.Evaluate(False)\n",
        "\n",
        "evaluator.SampleTopNRecs(ml)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading topic ratings...\n",
            "\n",
            "Computing topic popularity ranks so we can measure novelty later...\n",
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Evaluating  User KNN ...\n",
            "Evaluating accuracy...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "Evaluating  Item KNN ...\n",
            "Evaluating accuracy...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "Evaluating  Random ...\n",
            "Evaluating accuracy...\n",
            "Analysis complete.\n",
            "\n",
            "\n",
            "Algorithm  RMSE       MAE       \n",
            "User KNN   0.7006     0.4894    \n",
            "Item KNN   0.6349     0.4724    \n",
            "Random     0.8973     0.6721    \n",
            "\n",
            "Legend:\n",
            "\n",
            "RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\n",
            "MAE:       Mean Absolute Error. Lower values mean better accuracy.\n",
            "\n",
            "Using recommender  User KNN\n",
            "\n",
            "Building recommendation model...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "SCIENCE_AND_TECHNOLOGY 2.6129411764705894\n",
            "PERCENTAGE 2.3149999999999995\n",
            "CURRENT_AFFAIRS 2.1777992190346853\n",
            "SYLLOGISM 1.9926978729062577\n",
            "SIMILARITIES_AND_DIFFERENCES 1.9718750000000007\n",
            "ELEMENTARY_ALGEBRA 1.9063774746763715\n",
            "STATIC_GK 1.661017800949461\n",
            "ELEMENTARY_STATISTICS 1.599\n",
            "PROFIT_AND_LOSS 1.5814285714285714\n",
            "GEOGRAPHY 1.5521696627781403\n",
            "\n",
            "Using recommender  Item KNN\n",
            "\n",
            "Building recommendation model...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "MENSURATION 1.49548949223339\n",
            "SERIES 1.49548949223339\n",
            "PUZZLE 1.49548949223339\n",
            "ANALYTICAL_REASONING 1.49548949223339\n",
            "BIOLOGY 1.49548949223339\n",
            "CHEMISTRY 1.49548949223339\n",
            "DATA_INTERPRETATION 1.49548949223339\n",
            "DATA_SUFFICIENCY 1.49548949223339\n",
            "GEOMETRY 1.49548949223339\n",
            "MATHEMATICAL_OPERATIONS 1.49548949223339\n",
            "\n",
            "Using recommender  Random\n",
            "\n",
            "Building recommendation model...\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "ELEMENTARY_ALGEBRA 3.2994841464074325\n",
            "PHYSICS 2.784960370817352\n",
            "CURRENT_AFFAIRS 2.7297218563366283\n",
            "CODING_DECODING 2.481411828159202\n",
            "TRIGONOMETRY 2.479134446947376\n",
            "SIMILARITIES_AND_DIFFERENCES 2.449887284741753\n",
            "PUZZLE 2.3258959357101334\n",
            "STATIC_GK 2.2877991167430967\n",
            "DATA_INTERPRETATION 2.2426367135666005\n",
            "SCIENCE_AND_TECHNOLOGY 2.1602850362738675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQkte7nD7PIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4748b8a-5c77-4f02-ee88-2aa28bf976c0"
      },
      "source": [
        "#from ExamLounge import ExamLounge\n",
        "from surprise import KNNBasic\n",
        "import heapq\n",
        "from collections import defaultdict\n",
        "from operator import itemgetter\n",
        "        \n",
        "testSubject = '89'\n",
        "k = 10\n",
        "\n",
        "ml = ExamLounge()\n",
        "data = ml.loadExamLoungeLatestSmall()\n",
        "\n",
        "trainSet = data.build_full_trainset()\n",
        "\n",
        "sim_options = {'name': 'cosine',\n",
        "               'user_based': False\n",
        "               }\n",
        "\n",
        "model = KNNBasic(sim_options=sim_options)\n",
        "model.fit(trainSet)\n",
        "simsMatrix = model.compute_similarities()\n",
        "\n",
        "testUserInnerID = trainSet.to_inner_uid(testSubject)\n",
        "\n",
        "# Get the top K items we rated\n",
        "testUserRatings = trainSet.ur[testUserInnerID]\n",
        "kNeighbors = heapq.nlargest(k, testUserRatings, key=lambda t: t[1])\n",
        "\n",
        "# Get similar items to stuff we liked (weighted by rating)\n",
        "candidates = defaultdict(float)\n",
        "for itemID, rating in kNeighbors:\n",
        "    similarityRow = simsMatrix[itemID]\n",
        "    for innerID, score in enumerate(similarityRow):\n",
        "        candidates[innerID] += score * (rating / 5.0)\n",
        "    \n",
        "# Build a dictionary of stuff the user has already seen\n",
        "watched = {}\n",
        "for itemID, rating in trainSet.ur[testUserInnerID]:\n",
        "    watched[itemID] = 1\n",
        "    \n",
        "# Get top-rated items from similar users:\n",
        "pos = 0\n",
        "for itemID, ratingSum in sorted(candidates.items(), key=itemgetter(1), reverse=True):\n",
        "    if not itemID in watched:\n",
        "        topicId = trainSet.to_raw_iid(itemID)\n",
        "        print(ml.gettopicName(int(topicId)), ratingSum)\n",
        "        pos += 1\n",
        "        if (pos > 10):\n",
        "            break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "ANALOGY 3.6719999999999993\n",
            "ECONOMY 3.6719999999999993\n",
            "PHYSICS 3.5062702422533283\n",
            "AVERAGES 3.5021417220440387\n",
            "BLOOD_RELATION 3.4958845255155153\n",
            "HISTORY 3.48342929847253\n",
            "CODING_DECODING 3.346460180841013\n",
            "CURRENT_AFFAIRS 3.327414225379398\n",
            "NUMBER_SYSTEM 3.3128198555749124\n",
            "STATEMENT_AND_CONCLUSION 3.283754560178465\n",
            "TIME_AND_DISTANCE 3.269555874110374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la3DMHlX7PK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97430204-f9a3-46e8-e587-fe88a591e8ea"
      },
      "source": [
        "#from ExamLounge import ExamLounge\n",
        "from surprise import KNNBasic\n",
        "import heapq\n",
        "from collections import defaultdict\n",
        "from operator import itemgetter\n",
        "        \n",
        "testSubject = '89'\n",
        "k = 10\n",
        "\n",
        "# Load our data set and compute the user similarity matrix\n",
        "ml = ExamLounge()\n",
        "data = ml.loadExamLoungeLatestSmall()\n",
        "\n",
        "trainSet = data.build_full_trainset()\n",
        "\n",
        "sim_options = {'name': 'cosine',\n",
        "               'user_based': True\n",
        "               }\n",
        "\n",
        "model = KNNBasic(sim_options=sim_options)\n",
        "model.fit(trainSet)\n",
        "simsMatrix = model.compute_similarities()\n",
        "\n",
        "# Get top N similar users to our test subject\n",
        "# (Alternate approach would be to select users up to some similarity threshold - try it!)\n",
        "testUserInnerID = trainSet.to_inner_uid(testSubject)\n",
        "similarityRow = simsMatrix[testUserInnerID]\n",
        "\n",
        "similarUsers = []\n",
        "for innerID, score in enumerate(similarityRow):\n",
        "    if (innerID != testUserInnerID):\n",
        "        similarUsers.append( (innerID, score) )\n",
        "\n",
        "kNeighbors = heapq.nlargest(k, similarUsers, key=lambda t: t[1])\n",
        "\n",
        "# Get the stuff they rated, and add up ratings for each item, weighted by user similarity\n",
        "candidates = defaultdict(float)\n",
        "for similarUser in kNeighbors:\n",
        "    innerID = similarUser[0]\n",
        "    userSimilarityScore = similarUser[1]\n",
        "    theirRatings = trainSet.ur[innerID]\n",
        "    for rating in theirRatings:\n",
        "        candidates[rating[0]] += (rating[1] / 5.0) * userSimilarityScore\n",
        "    \n",
        "# Build a dictionary of stuff the user has already seen\n",
        "watched = {}\n",
        "for itemID, rating in trainSet.ur[testUserInnerID]:\n",
        "    watched[itemID] = 1\n",
        "    \n",
        "# Get top-rated items from similar users:\n",
        "pos = 0\n",
        "for itemID, ratingSum in sorted(candidates.items(), key=itemgetter(1), reverse=True):\n",
        "    if not itemID in watched:\n",
        "        topicId = trainSet.to_raw_iid(itemID)\n",
        "        print(ml.gettopicName(int(topicId)), ratingSum)\n",
        "        pos += 1\n",
        "        if (pos > 10):\n",
        "            break\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "TIME_AND_DISTANCE 7.113110620871188\n",
            "SIMILARITIES_AND_DIFFERENCES 5.935430840742821\n",
            "VENN_DIAGRAM 5.393622890779607\n",
            "STATEMENT_AND_CONCLUSION 4.7013127324932835\n",
            "STATIC_GK 2.9837062800057406\n",
            "GEOGRAPHY 2.011085758717615\n",
            "ECONOMY 0.7525110416155716\n",
            "ELEMENTARY_ALGEBRA 0.4853696218420437\n",
            "SYLLOGISM 0.31981719268661785\n",
            "HISTORY 0.2182282020685157\n",
            "ANALOGY 0.1900090380079319\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}